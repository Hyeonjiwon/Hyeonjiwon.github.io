---
title: '[Bigdata] 데이터 사이언스 소개 & Python 기초'
excerpt: 공공빅데이터 청년인턴십 온라인 교육 - 파이썬(판다스)으로 배우는 데이터 분석 입문
categories:
    - 

tag:

author_profile: true    #작성자 프로필 출력 여부

last_modified_at: 2021-06-01T17:00:00+09:00

toc: true   #Table Of Contents 목차 
toc_sticky: true
published : false
---

---

## 데이터 사이언스 소개 & Python 기초
- 데이터 분석이란?

데이터 분석(Data Analysis)은 유용한 정보를 발굴하고 결론을 알리며 의사결정을 지원하는 것을 목표로 데이터를 정리, 변환, 모델링하는 과정

오늘날 비즈니스 부문에서 데이터 분석은 의사 결정을 더 과학적으로 만들어주고 비즈니스를 더 효율적으로 운영할 수 있도록 도와주는 역할을 함

데이터 > 분석 > 의사결정 지원

- 필요성 

비즈니스 모델이 성숙도가 증가함에 따라 처리해야할 업무의 난이도와 복잡도가 증가

데이터 분석에 기반한 합리적이고 효율적인 의사결정의 중요서잉 대두됨

대량의 데이터를 손쉽게 구할수 있는 환경이 조성됨

- A/B 테스트

마케팅과 웹 분석에서 전체 또는 일부 요소를 변형시킨 A와 B안의 대조 실험을 통해서 각각의 안의 성능을 측정하는 방법론

- 데이터 사이언스 관련 직군

Data Scientist : 데이터에 대한 가설을 설정, 데이터를 분석, 분석한 데이터로 파악한 특징을 시각화해서 다른 사람들과 공유
 > 비즈니스에 대한 이해력 (도메인 지식) : 어떤 데이터를 분석할지, 분석한 데이터에 기반한 정보가 회사의 비즈니스에 어떤 도움이 될 수 있는지를 고민, 이를 반영하기 위해 회사에서 진행하는 비즈니스에 대한 이해력이 필요
 
 > 데이터 분석 능력 : 데이터 분석을 통해 분석하는 데이터의 중요 특징을 파악하고, 가설 및 가설 검증을 통해 데이터로부터 유의미한 결론을 도출해내야만 함

 > 데이터 시각화 능력 : 데이터 분석 결과를 그래프와 대시보드 등을 이용해 명확히 시각화 한 뒤, 이를 다른 팀원들과 공유할 수 있어야 함

Data Engineer : Data Science 프로젝트 진행을 위한 데이터 수집 및 전처리, 학습된 모델의 안정적인 배포를 위한 인프라 등을 개발

 > 데이터 크롤링 및 정제를 위한 프로그래밍 능력 : 데이터 크롤링을 통해 데이터를 수집하고, 수집한 데이터에서 필요한 부분만을 추출해서 정제할 수 있도록하는 프로그래밍 능력 필요

 > 모델 배포 및 API 구축을 위한 인프라 구성을 위한 엔지니어링 능력 : 학습된 모델을 배포하고 API 구축을 위한 인프로 구성을 위한 엔지니어링 능력 필요
 
 > Database 및 SQL 활용능력 : 데이터를 효율적으로 저장하기 위한 Database 활용 능력과 Database에 저장된 데이터를 분석하기 위한 SQL 활용능력 필요 

Machine Learning Engineer : Data Science 프로젝트 진행을 위한 새로운 머신러닝 모델을 개발
 
 > 최신 논문을 읽을 수 있는 능력
 > 모델 구현을 위한 프로그래밍 능력(TensorFlow, PyTorch)
 > 모델 성능 측정 및 개선 능력

- 프로그래밍의 개념
코딩/프로그래밍이란 프로그래밍 언어를 이용해 컴퓨터가 내가 원하는 기능을 수행할 수 있도록 명령을 내리는 것

요구사항 > 프로그래밍(코드 작성) > 컴퓨터가 요구사항을 수행

 > 데이터를 정제, 분석 시각화 하는 과정이 필요
 > 나만의 환경에 맞는, 내가 다루는 데이터에 꼭 맞는 분석을 수행하기 위해서는 코딩을 이용한 분석을 수행해야만 함
 > 회사마다 데이터가 구성되어 있는 형태는 모두 다르기 때문에 결국 코딩을 통한 전처리와 분석이 필수적

 > 프로그래밍 언어에 대한 문법
 > 자료구조 / 알고리즘
 > 로직 작성 능력

- Python
 1991년 귀도 반 로섬(Guido van Rossum)에 의해 제안된 프로그래밍 언어
 직관적인 문법을 갖고 있기 때문에 프로그래밍 입문자가 쉽게 접근할 수 있는 언어
 
- 변수(Variable)
포스트잇과 같이 임시적으로 데이터를 저장하거나 변경, 삭제할 수 있도록 기록해 놓을 수 있음

변수명 = 변수에 넣을 값 형태로 선언

print() 함수 : 함수 안에 출력하고 싶은 값을 입력하여 출력할 수 있음

- Python 기본 자료형
int(정수형)
float(실수형)
str(문자형)
bool(논리형)

- Python 기본 연산자
+, -
*, /
%
, >, >=, <, <=>

주석
한줄 주석(#)
여러줄 주석(''' ~ ''')

- 라이브러리란?
재활용이 가능한 기능들을 패키지 형태로 구성한 것

공통된 기능들을 재구현할 필요없이 라이브러리를 임포트해서 사용 가능

Pandas, Numpy, Scikit-learn, Matplotlib, Seaboarn 

Jupyter Notebook : 웹브라우저상에서 단계별로 파이썬 코드를 실행할 수 있도록 도와주는 프로그램

Anaconda : 데이터 분석을 위한 라이브러리들을 미리 설치해놓은 파이썬 배포판

- 자료구조 - list
자료구조 : 효율적인 데이터 처리를 위해 고안된 데이터 저장 구조

list : 여러 값을 순차적으로 저장해 놓을 수 있는 자료 구조

[데이터1, 데이터2, 데이터3]

리스트 생성([], list)
list1 = []
list2 = list()

값 추가(append, insert)
list1.append('100')
list2.append('a')
list2.insert(0, 'before_a')
print(list1)
print(list2)

특정 값 가져오기
list1[2]

특정 값 제거
list1.remove(100)

- for 반복문
list = [111, 222, 333]
for index in list:
    print(i, idx)
    i++

for index in range(0, 반복횟수):
    반복수행하고싶은 명령


## 데이터 분석을 위한 Python & Numpy

- Python의 장점
대용량 처리 가능
복잡한 기능을 쉽게
다른 용도로 확장 용이 - Web, 통계, 서버 개발, 엑셀 연동, 머신러닝, 이미지

- 함수(Function)

함수이름(인자1, 인자2, ..., 키워드1 = '기본값', 키워드2 = True, ...) 

괄호 안에 함수를 동작시킬 인자(INPUT)을 입력하면, 결과가 반환(return)

- 넘파이(Numpy) 라이브러리
Numerical Python 다차원 데이터를 쉽게 처리할 수 있게 도와주는 패키지

다차원 배열, 행렬의 생성과 연산, 정렬 등 편리한 기능들을 많이 포함하고 있음

각 차원의 순서 번호의 시작은 0

import numpy as np
// 2차원 배열 생성
array1 = np.array([[1, 2, 3, 4],[11, 12, 13, 14]])

// 배열 모양 보기
print(array1.shape)

- 다차원 배열 만들기
// 초기화 함수로 넘파이 배열 생성
z1 = np.zeros([ 4 ])
o1 = np.ones([ 3, 4 ])
r2 = np.random.randn(2, 3)

r4 = np.random.randn(2, 2, 4, 5).round(3)

- indexing으로 배열의 일부 추출하기
넘파이 배열의 일부를 가져오는 코드
[] 대괄호 하나 당 하나의 차원으로, 높은 차원부터 접근
my_np_arr[가져올 번호][가져올 번호]

// r4 배열에서, [ 두번째][ 첫번째 ] 접근해서 가져오기
r4[1][0]
r4[1][0][2]
r4[1][0][2][4] = r4[1, 0, 2, 4]

- slicing으로 배열의 일부 추출하기
넘파이 배열의 여러 개의 원소를 가져오는 코드
[] 대괄호 하나 당 각 차원에서 [시작, 시작 + 간격, 시작+ 2간격, ..., 끝-간격]에 해당하는 원소를 가져옴 (생략 시 기본값, 시작: 0, 끝: 길이, 간격: 1) 
가져오는 마지막 원소가 끝을 포함하지 않는 것을 주의
my_np_arr[시작:끝:간격][시작:끝:간격]...

r4[1][0][2][0:4:2]
r4[1][0][1:4:2]

// indexing과 slicing 같이 적용하기
r4[1][0][1:4:2][1][3:]

r4[1, 0][1:4:2][1,3:]
 
- 배열 간 사칙 연산
array1 = np.array([1, 2, 3, 4],[11, 12, 13, 14])
arr + arr 

- Broadcasting 연산 (Scalar)
// 덧셈 브로드캐스팅
arr + 20 

// 곱셈 브로드캐스팅
arr * 100

- Aggregation (집계)
넘파이 배열에 대해 모든 원소를 집계하는 연산

수학 : sum, mean, prod / 최대 최소 : max, min, argmax, argmin

axis 인자를 주어, 특정 축으로 집계할 수 있음

// 배열에서의 최대인 원소의 번호, 최소인 원소의 번호
arr.argmax(), arr[1].argmax(), arr.argmin()

// 특정 축(차원)으로 덧셈 수행하기
arr.sum(axis=0), arr.sum(axis=1)

// indexing + slicing + aggregation 응용
arr[1].min(), arr[1][2:].sum()


- 고급 Broadcasting 
arr1 + np.array()

## 판다스(Pandas)

- 판다스(Pandas)
데이터 표를 다루는 도구

- 시리즈(Series)
Series - 행 x 값 데이터

```python
import pandas as pd
my_series

// Series의 인덱스 보기
my_series.index

// Series의 이름 보기
my_series.name
```

1차원 데이터를 잘 표현하기 위해 만듬
데이터 배열에 이름과 각 데이터의 라벨(인덱스)를 붙임

indexing, slicing 하기 (값 접근)
.loc는 Series에서 인덱스를 이용하여 일부를 추출 (slice는 끝을 포함)

.iloc는 Series에서 배열번호를 이용하여 일부를 추출(slice는 끝을 포함 안함)

인자 또한 slice가 될 수 있다 (슬라이스: '처음:끝:간격'형태)

my_series.loc[인덱스 (or 인덱스 slice)]
my_series.iloc[배열번호 (or 배열번호 slice)]

broadcasting, aggregation

// Series 생성하기


- 데이터프레임(DataFrame)
행x열 데이터

2차원 데이터를 잘 다루기 위해 만듬

여러 개의 Series를 묶어서 만든 형태(즉, 하나 씩 떼어놓으면 Series)

각각 Seriesd의 name은 DataFrame에서는 열(Column)

indexing, slicing 하기 (값 접근)
.loc는 DataFrame에서 인덱스와 컬럼(열 이름)을 이용하여 일부를 추출 (slice는 끝 포함)

.iloc는 DataFrame에서 인덱스번호와  컬럼 번호를 이용하여 일부를 추출(slice는 끝 포함 안함)

인자 또한 slice가 될 수 있다 (슬라이스: '처음:끝:간격'형태)

추출할 shape가 (1, m) 혹은 (n, 1)일 경우 Series가 반환하며, 그 외  DataFrame을 반환

my_series.loc[인덱스 (or 인덱스 slice)]
my_series.iloc[배열번호 (or 배열번호 slice)]

// DataFrame 연산 총 정리
단일 연산
절대값 : .abs()
na 여부 : .isna()
유효여부 : .notna()
거듭제곱 : .pow()

축 방향 연산 (axis = 0 or 1)
평균 : .mean()
중앙값 : .median()
최댓값 : .max()
최솟값 : .min()
더하기 : .sum()
곱하기 : .prod()
최대원소의 인덱스 : .idxmax()
최소원소의 인덱스 : .idxmin()

누적 축 방향 연산 (axis = 0 or 1)
누적 최댓값 : .cummax()
누적 최솟값 : .cummin()
누적 곱셈 : .cumprod()
누적 덧셈 : .cumsum()

// DataFrame 정렬 (sort_values, rank)
my_df.sort_values(정렬기준, axis=축, ascending=True)
정렬기준의 행 또는 열 값들을 기준으로 axis 방향 정렬을 수행

axis : 정렬할 축, 0 또는 'index'는 행을 정렬하고 1 또는 'columns'는 열을 정렬
ascending : 오름차순 여부를 지정, 기본 값은 True, 내림차순은 False

my_df.rank(axis=cnr, ascending=True)
축의 방향을로 정렬, 각각의 행 또는 열들의 순위를 매김, 축과 오름차순 여부 지정 가능


## 4강 데이터 시각화를 위한 Matplotlib, Seaborn

https://gist.github.com/solaris33/f8e058643f89db6c62cbb618c1048ec1

- 데이터 시각화(Data Visualization)
데이터를 더 깊게 들여다 보거나, 분석 결과를 토대로 다른 사람들과 커뮤니케이션하기 위해서는 데이터를 그래프(graph) 형태로 시각화하는 데이터 시각화 과정이 필요

데이터 분석 결과를 그래프 형태로 만들어주는 대표적인 Python 라이브러리는 Matplotlib, Seaborn 

// python-graph-gallery.com

- Matplotlib
그래프를 그리거나, 분포를 보여주는 등 시각화를 위한 파이썬 패키지

다재다능하나 사용하기 약간 불편

- Seaborn
matplotlib을 감싸서 만든, 보다 쉬운 파이썬 시각화 패키지

다양하고 화려한 그래프를 matplotlib 보다 쉬운 코드로 그릴 수 있음

matplotlib의 명령어 사용 가능 

> Matplotlib 라이브러리 임포트
from matplotlib import pyplot as plt
%matplotlib inline

> x, y 축 데이터 준비

> plt.plot 으로 그래프 그리기
plt.plot([X축 데이터], [Y축 데이터])


기본 bar 차트 그리기 

> 라이브러리 임포트 및 데이터 준비
복잡한 바 차트는 될 수 있으면 seaborn 이용 추천

> 산점도 (scatter) 차트 그리기 
plt.scatter(x_data, y_data)
plt.show()

> matplotlib으로 그래프 그리기
pandas에는 matplotlib으로 그래프를 그리는 기능이 내장되어 있음

Series.plot(), DataFrame.plot() 
plot() 함수 인자로 다양한 옵션 존재

seaborn으로 데이터 분포 살펴보기


```python
import seaborn as sns
sns.set()
plt.rc('font', family='')

// 내장 데이터 셋
tips = sns.load_dataset('tips')

// dataframe의 앞부분만 살펴보기
tips.head()

// 데이터 수 세기
sns.countplot(data=tips, x='day');
plt.title('요일별 팁을 준 횟수');

// 4분위 도표 boxplot
// 두 변수 관계를 점찍어 그리기 scatterplot
// 두 변수 관계를 점과 분포로 보기 joinplot
// hue 인자로 boxplot 그래프 분리
// 빈도를 바이올린 모양으로 viloinplot
// 데이터의 분포를 그리는 displot
```

## 5강 Kaggle Titanic 사고 데이터로 시작하는 데이터 분석

- 캐글(Kaggle)
data scientist를 위한 데이터 분석 및 예측 경진대회 플랫폼

데이터 분석 프로세스 
수집(Obtain) > 정제(Scrub) > 탐색(Explore) > 모델(Model) > 해석(Interpret)

- EDA(Exploratory Data Analysis)
탐색적 데이터 분석은 본격적인 데이터 분석의 첫 단계로 데이터의 모양과 크기, 분포, 빈 값 등 데이터가 어떠한 형태인지 관찰하고 파악하는 단계

데이터로 실험 하기 전, 데이터를 보다 잘 이해하기 위한 단계

목적
데이터의 오류가 없는지 확인 
각 데이터의 용도나 설명, 의미를 파악
데이터의 개별 속성에 대해 파악

작업
값의 종류 파악하기(숫자, 카테고리, 순서 등)
숫자 형 데이터 최소, 최대, 평균 살펴보기
이상치(Outlier) 및 누락 유무 확인하기 
변수간 관계 분석하기, 분포 그려보기
합치거나 쪼개서 유의미한지 확인하기 

- 1912 타이타닉 사고 데이터
타이타닉 사고의 사망자, 생존자를 예측하는 Competition으로, 탑승객의 정보를 이용하여 예측

데이터 다운로드 : seaborn의 .load_dataset('titanic')을 이용하여 불러올 수 있음

891명의 승객에 대한 데이터 
생존여부 / 좌석 등급 / 성별 / 나이 / 일행 / 자녀 / 운임 등의 feature 제공

EDA 목표
주어진 각 feature들의 분포 살펴보기, 생존자 / 사망자 별로 데이터를 분리하여 살펴보기 

어떤 정보를 통해 생존율을 예측할 수 있을 지, 가설을 세우고 실제 그래프로 검증해보기

- Training Data, Validation Data, Test Data



## 6강 예측 모델링을 위한 scikit-learn & 머신러닝 기초

- 머신러닝(Machine Learning)
명시적인 프로그래밍 없이 데이터를 이용해서 예측 또는 분류를 수행하는 알고리즘을 구현하는 기법을 뜻함

데이터 > 머신러닝 모델 > 예측 또는 분류

필요한 이유
인간이 정확히 하나하나 로직을 지정해주기 어려운 복잡한 문제를 데이터에 기반한 학습을 통해서 해결 가능

머신러닝 알고리즘을 사용할때 가장 중요한 부분은 머신러닝 모델이 잘 학습할 수 있도록 적절한 특징(Feature)을 설정해주는 것

예측 모델(Prediction Model)의 필요성
데이터 분석을 통한 정교한 예측 모델을 갖고 있을 경우, 중요한 비즈니스적 의사결정을 안정적이고 계획적으로 수행할 수 있음

지도 학습(Supervised Learning)
머신러닝 모델은 일반적으로 지도 학습이라는 방법론을 사용

트레이닝 데이터의 구성이 (input data, 데이터에 대한 정답) 쌍으로 구성되어 있어야 함

지도 학습은 정답을 보여주면서 학습시키는 머신러닝 방법론

인풋 데이터 : x, 데이터에 대한 정답 : y
데이터는 (x, y)쌍으로 구성

ex) 키를 기반으로 몸무게를 예측하는 모델 > 트레이닝 데이터는 (키, 몸무게)

- Training Data, Validation Data, Test Data

Training Data, Test Data
(x, y)로 구성된 데이터를 학습용 데이터(Training Data)라고 부름
머신러닝 모델을 사용하는 경우 다음의 2가지 과정을 거침

1 학습용 데이터로 머신러닝 모델을 학습시킨다.
2 학습된 머신러닝 모델의 성능을 트레이닝 데이터에 포함되어 있지 않고 따로 빼놓은 테스트 데이터(Test Data)로 측정

트레이닝 데이터, 테스트 데이터 나누기(split)
머신러닝 모델을 학습시키기 위해서 전체 데이터의 일부를 Training Data, 일부는 Test Data로 나눠서 사용
일반적으로 데이터의 80% 정도는 트레이닝 데이터, 20% 정도는 테스트 데이터로 나눠서 사용
예를 들어 1000명의 (키, 몸무게) 데이터가 있다면 800명분의 데이터는 트레이닝 데이터, 200명분의 데이터는 테스트 데이터로 나눠서 사용

Validation Data(검증용 데이터)
전체 데이터를 트레이니 데이터, 검증용 데이터, 테스트 데이터로 나누기도 함
검증용 데이터는 트레이닝 과정에서 학습에 사용하지는 않지만 중간중간 테스트하는데 사용해서 학습하고 있는 모델이 오버피팅에 빠지지 않았는지 체크하는데 사용
즉, 검증용 데이터는 트레이닝 과정 중간에 사용하는 테스트 데이터로 볼 수 있음

- Overfitting, Underfitting
처음에는 트레이닝 에러와 검증 에러가 모두 작아지지만 일정 횟수 이상 반복할 경우 트레이닝 에러는 작아지지만 검증 에러는 커지는 오버피팅(Overfitting)에 빠지게 됨
트레이닝 에러는 작아지지만 검증 에러는 커지는 지점에서 업데이트를 중지하면 최적의 파라미터를 얻을 수 있음 (Early Stopping)

오버피팅 : 학습 과정에서 머신러닝 알고리즘의 파라미터가 트레이닝 데이터에 과도하게 최적화되어 트레이닝 데이터에 대해서는 잘 동작하지만 새로운 데이터인 테스트 데이터에 대해서는 잘 동작하지 못하는 현상, 표현력이 지나치게 강력할 경우 발생하기 쉬움

언버피팅 : 오버피팅의 반대 상황으로 모델의 표현력이 부족해서 트레이닝 데이터도 제대로 예측하지 못하는 상황을 말함

Regularization : 오버피팅을 방지하기 위한 기법들

예측 모델 값의 형태에 따라서 분류 문제 혹은 회귀 문제로 나뉨
분류(Classification) 문제 : 예측하는 결과값이 이산값(Discrete Value)인 문제
ex) 이 이미지에 해당하는 숫자는 1인가 2인가?
회귀(Regression) 문제 : 예측하는 결과값이 연속값(Continuous Value)인 문제
ex) 3개월 뒤 이 아파트 가격은 2억 1천만 원일 것인가? 2억 2천만 원일 것인가?

- 선형회귀 Linear Regression
선형회귀 모델은 아래와 같은 선형 함수를 이용해서 회귀(Regression)를 수행하는 모델을 뜻함
> y = Wx + b
이때 x, y는 가지고 있는 데이터이고, w와 b는 데이터에 적합한 값으로 학습될 수 있는 파라미터(Parameter)

손실 함수(Loss Function) - MSE
머신 러닝 모델을 학습시키기 위해서는 적절한 파라미터값을 알아내기 위해서 현재 파라미터값이 우리가 풀고자 하는 목적에 적합한 값인지를 측정할 수 있어야 함
이를 위해 손실함수를 정의
손실 함수는 여러가지 형태로 정의될 수 있음
그 중 가장 대표적인 손실 함수 중 하나는 평균제곱오차(MSE : Mean of Squared Error)
손실 함수는 우리가 풀고자 하는 목적에 가까운 형태로 파라미터가 최적화 되었을때(즉, 모델이 잘 학습되었을 때) 더 작은 값을 갖는 특성을 가져야만 함
이런 특징 때문에 손실 함수를 다른 말로 비용 함수(Cost Function)라고도 부름

- scikit-learn
다양한 머신러닝 모델을 쉽고 간편하게 구현할 수 있도록 도와주는 라이브러리

기본 사용법
1 Estimator 선언(ex. LinearRegression)
2 .fit() 함수 호출을 통한 트레이닝(Training)
3 .predict() 함수 호출을 통한 예측(Predict)

scikit-learn을 이용한 예측 모델 구성방법
트레이닝 데이터, 테스트 데이터 준비 > Feature 정규화(Normalize) > Estimator 선언 > .fit() > .predict()

scikit-learn을 training data, test data 나누기
> form sklearn.model_selection import train_test_split
> #80는 트레이닝 데이터, 20%는 테스트 데이터로 나눔
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

scikit-learn을 선형회귀(Linear Regression) Estimator 선언하기
> form sklearn.model_selection import LinearRegression
> #선형회귀(Linear Regression) 모델 선언하기
> lr = LinearRegression()

scikit-learn을 이용해서 MSE, RMSE 정의하기
MSE는 차이를 제곱하기 때문에 제곱에 의해서 생기는 오차를 보정하기 위해 RMSE(Root Mean Square Error)를 이용해서 성능을 측정하기도 함
> form sklearn.model_selection import mean_squared_error
> #MSE(Mean Squared Error)를 측정
> MSE = mean_squared_error(y_test, y_preds)
>
> #RMSE(Root Mean Squared Error)를 측정
> RMSE = np.sqrt(MSE)

[scikit-learn을 이용해서 상황에 따른 적절한 모델(Estimator) 선택하기](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)


## 7강 Kaggle 보스턴 부동산 가격 예측해보기
- 보스턴 부동산 가격 데이터
실습 데이터는 1970년도의 보스턴 지역의 집값을 나타내는 데이터
선형 회귀(Linear Regression) 알고리즘을 이용해서 보스턴 부동산 가격을 예측 해보기

보스턴 부동산 가격 데이터 특징들(Features)
CRIM : 도시별 범죄발생률
ZN : 25,000평을 넘는 토지의 비율
INDUS : 도시별 비상업 지구의 비율
CHAS : 찰스 강의 더미 변수 (1 = 강의 경계, 0 = 나머지)
NOX : 일산화질소 농도
RM : 주거할 수 있는 평균 방의 개수
AGE : 1940년 이전에 지어진 주택의 비율
DIS : 5개의 고용지원센터까지의 가중치가 고려된 거리
RAD : 고속도로의 접근 용이성에 대한 지표
TAX : 10,000달러 당 재산세 비율
PTRATIO : 도시별 교사와 학생의 비율
B : 도시의 흑인 거주 비율
LSTAT : 저소득층의 비율
MEDV : 본인 소유 주택 가격의 중앙값

Feature 정규화(Normalize)하기
Feature를 정규화할 경우, 값의 분포를 정규 분포(Normal Distribution)형태로 변경할 수 있음
일반적으로 Feature값에 대한 정규화를 수행할 경우, 더 안정적으로 머신러닝 모델을 학습시킬 수 있음

StandardScaler 클래스를 이용해 Feature 정규화 하기
> from sklearn import preprocessing
> X = boston_housde_df.iloc[:,:-1]
> #StandardScaler를 이용해서 데이터 정규화(Normalize)하기
> X = preprocessing.StandardScaler().fit(X).transform(X)

상관 분석(Correlation Analysis)
상관 분석 또는 '상관관계' 또는 '상관'은 확률론과 통계학에서 두 변수간에 어떤 선형적 또는 비선형적 관계를 갖고 있는지를 분석하는 방법
> corr = boston_housde_df.corr()
> plt.figure(figsize=(10,10));
> sns.heatmap(corr, vmax=0.8, linewidths=0.01, square=True, annot=True, cmap='YlGnBu');
> plt.title('특징들간의 Correlation');

sns.regplot으로 Feature들 간의 경향성 출력해보기
sns.regplot(data={dataframe}, x={컬럼명}, y={컬럼명}) 형태를 이용해서 regression line이 포함된 scatter plot을 그릴 수 있음
> sns.regplot(data=boston_housde_df, x='RM', y='PRICE') 
> plt.show()


## 8강 랜덤 포레스트(Random Forest) & Kaggle 월마트 상품 판매량 예

- 데이터 클렌징(Data Cleansing)
데이터에서 유의미한 내용들만 골라내기 (데이터 청소하기)
데이터에 존재하는 이상치(outlier)나 결측치(Missing value)를 제거해서 데이터를 정리하는 것을 의미

- Feature Engineering
도메인 지식이나 분석을 통해서 유의미한 특징(Feature)들만을 선별해내거나 Feature의 형태를 더욱 적합한 형태로 변경하는 것

적절한 Feature Engineering 머신러닝 모델의 성능에 큰 영향을 끼칠 수 있음

Feature Selection : 유의미한 Feature들만 선별해서 남기는 것, 불필요한 Feature 제거
Normalization : 전체 데이터가 들어왔을 때 평균과 표준편차를 이용해 0~1 사이의 값으로 정규화 해주는 방법, 특정 값으로 정규화 하는 것 , 다양한 값의 범위가 공통된 값의 범위로 바뀜
Feature Generation : 새로운 Feature를 도메인 지식으로 만드는 것 

상관 분석 기법 많이 이용 
연관관계가 있는 특징인지 아닌지 확인할 수 있음
상관  분석 결과를 통해 필요 없는 특징 제거 

- 결정 트리(Decision Tree)
데이터 마이닝에서 일반적으로 사용되는 방법론
몇몇 입력 변수를 바탕으로 목표 변수의 값을 예측하는 모델을 생성하는 것을 목표로 함

트리 구조에서 각 내부 노드들은 하나의 입력 변수에,
자녀 노드들로 이어지는 가지들은 입력 변수의 가능한 값에 대응

잎 노드는 각 입력 변수들이 루트 노드로부터 잎 노드로 이어지는 경로에 해당되는 값들을 가질 때의 목표 변수 값에 해당

- 랜덤 포레스트(Random Forest)
분류, 회귀 분석 등에 사용되는 앙상블 학습 방법의 일종으로, 훈련 과정에서 구성한 다수의 결정 트리로부터 부류(분류) 또는 평균 예측치(회귀 분석)를 출력함으로써 동작

전체 데이터의 일부를 샘플링한 서브 데이터를 이용해서 학습시킨 여러개의 결정트리의 예측값들간에 보팅을 통해서 최종 출력값을 만들어내는 기법

- 앙상블 러닝 (Ensemble Learning)
여러 개의 분류기의 예측 결과값 간의 투표를 통해서 최종 결과값을 만들어내는 기법

더욱 좋은 예측 성능을 기대할 수 있음

scikit-learn에서 랜덤 포레스트를 구현한 Estimator
sklearn.ensemble.RandomForestClassifier - 분류(Classification)문제에 사용
sklearn.ensemble.RandomForestRegressor - 회귀(Regression)문제에 사용

- 랜덤 포레스트의 하이퍼 파라미터
하이퍼 파라미터(hyper-parameter)란 알고리즘의 동작 과정에 영향을 미치는 중요한 값들로써 알고리즘 디자이너가 결정해줘야만하는 값들

1. n_estimators : 랜덤 포레스트에서 사용할 결정트리 개수를 지칭, 기본값은 100개, 많이 설정 할수록 성능이 향상될 수 있지만 학습 시간이 오래걸릴 수 있음
2. max_features : 결정트리 분할 기준으로 사용할 Feature 개수
3. max_depth : 트리의 최대 깊이, 너무 깊어지면 오버피팅이 발생할 가능성이 있음 
4. min_samples_split : 노드를 분할하기 위한 최소한의 샘플 데이터수, 너무 작은 경우 과적합이 발생할 가능성이 높아짐, 기본값 2

- Kaggle 월마트 상품판매량 예측
월마트에서 데이터 사이언티스트를 채용하기 위해 올린 Kaggle Competition
월마트의 매점별 판매량 데이터 예측을 통해 최적의 재고관리 전략을 수립해보기

월마트 상품 판매량 판매량 수요예측 문제 정의
문제 정의 : 다른 지역에 있는 45개의 월마트 상점의 부서별 판매량 데이터를 통해 부서별 미래 판매량을 예측해보자 (각 상점별로 여러개의 부서가 있음)
판매량에 영향을 미치는 주요 특징 : 공휴일 기간에 시행하는 가격인하 프로모션 이벤트(Markdown event)
판매량에 큰 영향을 미치는 슈퍼볼(Super Bowl), 노동절(Labor Day), 추수감사절(Thanksgiving), 크리스마스(Christmas) 4개의 주요 공휴일이 있고, 이 날들이 포함된 주는 성능 측정시 5배의 가중치를 가짐

성능 평가 방법
weighted mean absoluter error(WMAE)를 이용해 성능을 평가

데이터 설명(Data Description)
stores.csv : 익명화된 45개 상점의 크기(size)와 타입(type)을 나타냄

train.csv : 2010년 2월 5일부터 2012년 11월 1일까지의 트레이닝 데이터를 나타냄
컬럼은 다음과 같음
1. Store : 스토어를 나타내는 숫자
2. Dept : 부서를 나타내는 숫자
3. Data : 날짜
4. Weekly_Sales : 부서별 주간 판매량 
5. IsHoliday : 공휴일 포함 유무

test.csv : 주간 판매량(Weekly_Sales)을 제외하고는 train.csv와 동일 형태의 파일
이 파일의 주간 판매량(스토어, 부서, 날짜별)을 예측하는 것이 Competiton의 목적

features.csv : 추가적인 특징정보를 포함
주요 컬럼들은 다음과 같음
1. Store : 스토어 숫자
2. Date : 날짜
3. Temperature : 상점이 위치한 지역의 평균 온도
4. Fuel_Price : 상점이 위치한 지역의 기름값 
5. MarkDown1-5 : 익명화된 월마트의 프로모션 데이터, 모든 기간에 데이터가 존재하는 것은 아니며 결측치는 NA로 표기되어 있음
6. CPI : 소비자 물가지수
7. Unemployment : 실업률
8. IsHoliday : 공휴일이 포함된 주인지 아닌지를 나타냄

각 공휴일에 대응되는 날짜는 다음과 같음
1. Super Bowl : 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13
2. Labor Day : 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13
3. Thanksgiving : 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13
4. Christmas : 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13

[8강 강의자료](https://gist.github.com/solaris33/38e7f566e4fe98944fe6602040ea16f6)