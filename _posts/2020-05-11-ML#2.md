---
title: '[머신러닝] 결정 트리(Decision tree)' 
excerpt: '결정트리 이론'
categories:
    - Machine Learning

tag:
    - ML
    - Decision tree

author_profile: true    #작성자 프로필 출력 여부

last_modified_at: 2020-05-11T22:00:00+09:00

toc: true   #Table Of Contents 목차 

toc_sticky: true
---

## 결정 트리 (Decision tree)

결정 트리(Decision tree)는 의사 결정 규칙과 그 결과들을 트리 구조로 도식화한 의사 결정 지원 도구의 일종입니다. 의사 결정 분석에서 목표에 가장 가까운 결과를 낼 수 있는 전략을 찾기 위해 주로 사용됩니다. 또한, 분류(Classification)와 회귀(Regression) 모두 가능한 지도 학습(Supervised learning) 모델 중 하나입니다.

![decision tree](https://user-images.githubusercontent.com/47733530/81569655-86168d80-93da-11ea-97b8-b44213bca96d.png)

위의 그림은 결정 트리를 도식화한 그림입니다. 트리에서 최상위 노드를 Root Node라고 하고, 자식 노드가 없는 가장 최하위 노드를 Leaf Node, Treminal Node 혹은 External Node 라고 합니다. 자식 노드를 가지고 있는 노드를 Internal Node라고 합니다. 


![dtex](https://user-images.githubusercontent.com/47733530/82053056-a8b8e700-96f7-11ea-8446-e1ef62d17a04.png)


위의 그림은 생존인지 사망인지 의사결정 해주는 decision tree입니다. 구조를 보면 Internal node는 "조건문"이고, 예시로는 성별 == 남자, 나이 > 9.5, sibsp > 2.5 조건문으로 나타나 있습니다. Edge는 조건 결과에 따른 분기로 yes 인 경우면 왼쪽으로, no 인 경우면 오른쪽 노드로 내려갑니다.
External node는 결과(사망 or 생존)를 보여줍니다. 
 

## 생성 알고리즘

성능이 좋은 Decision tree를 위해서는 좋은 트리의 조건을 고려해 주어야 합니다.


__좋은 트리의 조건__

1. Leaf node에서 통일된 label(class)의 데이터만 남는 것 -> 이는 높은 분류 정확도와 의사결정 정확도를 의미합니다. 

2. 트리의 높이(혹은 깊이)가 짧은 것 -> 트리의 높이(깊이)가 짧을수록 수행 속도가 빨라집니다.  

어떤 feature를 먼저 고려하는가에 따라 트리의 깊이가 달라질 수 있습니다. 좋은 트리의 조건을 모두 만족시키려면 어떤 방법으로 트리를 생성 하면 될까요? 


__트리 생성 방법__

트리를 생성할 때, 순서는 Root node에서 leaf nodes 순서로 행해져야 합니다. Root 노드는 모든 데이터를 고려한다고 가정하며 하위 노드(자식 노드)로 내려가면서 데이터들이 분류됩니다. 

1. 노드에서 고려할 데이터가 이미 하나의 class에만 속하였거나, 더 이상 고려할 feature가 없으면 Leaf node로 여기고 자식노드를 생성하지 않습니다. 

-> 고려할 feature가 더 이상 없어서 leaf node가 된 경우, 가장 많은 수의 calss를 결과 값으로 채택합니다. 

2. 각 노드에서 고려할 feature 선택 시, 데이터들을 __가장 잘 나눠주는__ feature를 선택합니다. 이때 Leaf node에서 완벽하게 데이터들이 나눠진다는 보장은 없습니다. 

3. 선택된 feature에 대한 조건 별로(값마다) 자식노드를 생성합니다. 

4. 각 자식 노드에서는 해당 조건을 만족하는 데이터만 고려하여 1부터 반복합니다. 

트래를 생성하면서 


수정 중...

## 

## 참고 자료