---
title: '[머신러닝] ANN(Artificial Neural Network)' 
excerpt: '인공 신경망 이론'
categories:
    - Machine Learning

tag:
    - ML
    - ANN
    - Artificial Neural Netowork

author_profile: true    #작성자 프로필 출력 여부

last_modified_at: 2020-05-20T15:00:00+09:00

toc: true   #Table Of Contents 목차 

toc_sticky: true
---

## ANN 학습 : Optimization
- Backpropagation 알고리즘 기반으로 편미분하여 얻은 수식들을 사용하여 실제로 업데이트를 수행하는 '최적화 알고리즘' 필요 
- 파라메터 업데이트 수식들을 적용할 때는 반대방향으로 적용(즉, 뺄셈)
- 알고리즘들
  - __Gradient Descent(GD)__
    - Batch GD, MiniBatch GD
  - __Adagrad__
  - Adadelta
  - __Adam's optimizer__

## 대표적인 Optimizer 'Gradient Descent'
- Cost function에 대한 편미분으로써의 파라메터 업데이트 수식들을 반대 방향으로 적용
- Iterative 방식
- 얼마나 내려갈지 learning rate로 조절

- Batch GD : 학습 Data가 N개 있을 때, N개 전체에 대한 grdient를 적용하는 것 
    -> N개 데이터가 메모리에 올라와 있어야 한다는 의미, 빅데이터일 경우 어려움
- Stochastic GD : 학습 Data N개 각각에 대한 gradient를 적용하는 것
- Mini-batch GD : 학습 Data N개를 작으느 mini-batch들로 쪼개서 gradient를 적용하는 거ㅛ

- 배치 사이즈 작은게 좋은지 큰게 좋은지는 아무도 모름 
- 통상적으로 많이 쓰이는 배치 사이즈는 100개, 50개, 32개, 64개, ... 이런식으로 정하는 경우가 많다
- learning rate도 처음에는 크게 하다가 점점 작게 learning rate DK(?)

## ANN 학습 : Optimizers
- 최적화 함수를 사용해도 Global optima 가 어디인지 모름
- 찾았다고 하더라도 그게 Global optima인지 모름 

## 실전 ANN : 전처리 (초기값)
- 초기값에 따라 결과가 달라짐
- 데이터 입력시 전처리 : 데이터의 각 feature 값을 scaling

- 파라메터(Layer 간의 weight matrix) 초기값
    - 0으로 초기화 : Backpropagation 진행 시, 모두 같은 양 만큼 gradient가 계산됨
- Normal(0, 0.01^2)
- Xavier 초기화 방법
- He 초기화 방법

- 초기화는 아직도 어떤 것이 제일 좋은지 모름,  사람의 경험에 따라 다름

## 실전 ANN : 오버피팅
- 오버피팅(overfitting) 피하기
    - 데이터 늘리기
    - Feature 줄여보기(파라메터가 줄어듬)
    - 모델 복잡도 줄여보기(노드 개수, 층 개수)
    - 일반화(regularization)

- hyper-parameter(학습할 대상이 아닌 사람이 강제로 주는것, ex. 노드 개수 등)를 결정함으로써 모델의 복잡도를 결정할 수 있으며,
이를 어떻게 설정하느냐에 따라 오버피팅을 피할 수 있다는 의미
- 적당한 횟수의 epoch도 오버피팅을 막을 수 있음
- hyper-parameter 튜닝 방법 : Grid-search, Random Search, 자동으로 찾아가기(Neural architecture Search), ...

## 실전 ANN : L1, L2 정규화
- L1 regularization 과 L2 regularization 비교 : L2가 자주 쓰임


수정중...
`
## 참고 자료
> 머신러닝 수업자료_ANN(순천향대학교 빅데이터공학과 정영섭 교수님)